Username2,LoginID2,Password2,navigateTo1,Project_name1,Segment_Name2,navigateTo2,Segments,Segment_Name1,Details_segment_fields,SPA_name,SPA_name_value,SPA,SPA_metrics_columns,SPA_metrics,table1,rowname1,colname1,BP_Value,table2,rowname2,colname2,EP_Value,BP_selectValue,selectbox_values1,EP_selectbox_Value,selectbox_EPvalues1,colname_Score1
SolomonDeepak_AI,SolomonDeepak_AI,111111,Projects_home,OVERCORP,Workflow,Self Project Assessment, Details;Self Project Assessment[2];Workflow;Previous Versions;Activity Log,Details,ID :;SPA1;Name * :,CM_Name,_Self-Project Assessment,Self Project Assessment[2],Practices;Maturity Level of Practices;Score,Agile Maturity Assessment Questionnaire;Part 1 - Behavioral Practices;Part 2 - Engineering Practices,Part 1 - Behavioral Practices,Project Setup|Backlog Grooming|Estimation|Work items Sizing|Team Composition|Planning|Daily Stand ups|Collaboration|Continuous Feedback|Continuous Improvement|Agile Metrics|Agile Charting,Practices_lbl,Project Setup|Backlog Grooming|Estimation|Work items Sizing|Team Composition|Planning|Daily Stand ups|Collaboration|Continuous Feedback|Continuous Improvement|Agile Metrics|Agile Charting,Part 2 - Engineering Practices,Configuration Management|Automation - Unit Test|Automation - Functional Test|Automation - Regession Test|Automation - Built in Quality|Continous Integration|Virtualised Services|Automated Deployments|Functional Smoke Testing|Release Frequency,Practices_lbl,Configuration Management|Automation - Unit Test|Automation - Functional Test|Automation - Regession Test|Automation - Built in Quality|Continous Integration|Virtualised Services|Automated Deployments|Functional Smoke Testing|Release Frequency,1. Team clearly articulates Defintion of Done and create release plan as part of its Project plan with all team members participating and with Client sign off#2. Team articulates Defintion of Done as part of its project plan at high level with some team members participating and with client sign off#--None--#3. Management articulates Defintion of Done at the high level and with or without client sign off.#4. NO Agreed Definition of Done.|--None--#1. Product Backlog with User Stories groomed and prioritsed by a client nominated Product Owner regularly. ALM tool like Rally Mingle RTC used#2. Product Backlog with User Stories groomed and prioritised by Product Owner regularly. Excel / Word based tracking done#3. Product Backlog exists but NOT updated and groomed regularly or are done so regularly by a Psuedo Product Owner / BA#4. No Product Backlog exists|--None--#1. Estimation in story points done with all team members using repeatable estimation methodology#2. Estimation in story points done with all key team members with gut-feel and previous experience#3. Estimation in story points done but key dev and testing team members are not involved.#4. No story points based estimation done.|--None--#1. Work items are broken down to smallest development size with minimum 80% completed in 1 day no more than 10% within 2 days and no more than 10% within 3 days.#2. Work items are broken down with over 60% being completed in 1 day 20% completed in 2 days and no more than 30% completed in 3 days.#3. Work items are broken down so that less than 50% are completed in 1 day.#4. Work items usually take more than 1 week to complete|--None--#1. Teams are composed of Core Discovery Roles of Product Owner Scrum Master Tech Lead - alongside Dev and testers - and are committed 100% to the Project with team size not exceeding 9 members#2. Teams are composed of Product Owner Scrum Master Tech Lead Dev and Test and committed 80% to 100%#3. Teams are composed of Psuedo Product Owner Scrum Master Tech Lead Dev and Test and committed 60% to 80%.#4. Teams are mixed and committed less than 60% to the Project team members count vary and go above 9|--None--#1. Beginning of every sprint sprint Planning meeting conducted with all team members involved. Team members pick up user stories.#2. Planning meeting conducted but with key leads PM and IM. User Stories are assigned to team members by leads / PM / IM.#3. Planning done on need basis. Not done regularly prior to sprint.#4. No sprint planning meeting conducted|--None--#1. Common Stand ups happen daily with all project team members across distributed locations including key client stakeholders with Product Owner as mandatory#2. Stand ups happen daily but separately at each location with not all team members joining together#3. Stand ups happen but NOT daily#4. Project status meeting happens. No stand ups.|--None--#1. Distributed Agile teams work collaboratively in a seamless manner using tools(e.g. Webex Wiki. ) for all agile ceremonies.#2. Distributed Agile teams work independently and deliver respective features. No cross feature team collaboration happens.#3. Teams are distributed. Collaborative Communcation happens only on few occasion / ceremonies.#4. Teams are distributed and communication is chaotic and mainly when there are issues.|--None--#1. Working Features are demonstrated to IT & Business stakeholders every sprint feedback recorded and acted upon#2. Working Features are demonstrated to IT & Business stakeholders every 2 sprints or irregularly.#3. Working Features are demonstrated to IT stakeholders irregularly.#4. Client gets to see the working features only during UAT phase|--None--#1. Retrospection done at the end of every sprint considered mandatory for all team members. Actions completed resulting in measurable continuous project improvement.#2. Retrospection done at the end of every sprint and actions taken occassionally or not all team members attend.#3. Retrospection meetings conducted but only few team members involved. Actions are not tracked to closure.#4. No Retrospeciton done.|--None--#1. Agile Metrics (e.g. Velocity) captured regularly. Used for Decision making actions and consistent improvements shown#2. Metrics captured regularly and occasional actions taken.#3. Metrics captured occassionally for reporting purposes. No evidence of actions based on metrics.#4. No metrics captured.|--None--#1. sprint Burn Down Chart (updated daily) Release Burn Up Chart Project Burn Up Chart and Cumulative Flow Diagram used and displayed visually (BVC) and also via shared team rooms visually#2. sprint Burn Down Chart (updated daily) and Release Burn Up Chart used but not prominently displayed or referenced internally#3. sprint Burn Down/Burn up Charts prepared for governance purposes but not updated daily.#4. No Charts captured or displayed.,Maturity Level of Practices_lst,1. Up-to-date Configuration Management Plan available with 100% of components used to deploy and test an application stored versioned and managed by an SCM system. Details of specific components and package configurations can be availed within 2 hours#--None--#2. CM Plan available with source code version controlled. But not all component and package configurations are available.#3. CM Plan available but not updated regularly. CM practices are followed irregularly.#4. CM Plan unavailable.|--None--#1. Automated Unit Test Coverage 100% capturing code coverage and recording /reporting unit test results#2. Automated Unit Test Coverage > 50%#3. Automated Unit Test Coverage < 50%#4. No Unit test automation|--None--#1. Functional Test Cases automated 100% wherever technically possible. Minimal Manual Testing.#2. Functional Test Cases automated wherever technically possible >60%.#3. Functional Test Cases automation wherever technically possible <60%#4. No / Minimal Test automation|--None--#1. Regression Test Cases automated 100% wherever technically possible. Minimal Manual Testing.#2. Regression Test Cases automated wherever technically possible >60%.#3. Regression Test Cases automation wherever technically possible <60%#4. No / Minimal Test automation|--None--#1. Code Quality & Code Coverage tools intergrated with CI tool and executed automatically. Results are anlaysed and addressed as part of iterations.#2. Code Quality & Code Coverage tools integrated with CI tool and executed automatically. Results are analysed and addressed based on time available.#3. Code Quality & Code Coverage tools are used but on need basis manually.#4. No Code Quality & Coverage tools are used|--None--#1. Continuous Integration implemented end to end from build test QA to deployment automated. Can be deployed to multiple dev / test environments.#2. Continuous Integration implemented end to end but deployment to various environments require manual steps.#3. Continuous Integration done in parts but not end to end.#4. No Continous Integration tool implemented.|--None--#1. Service Virtualisation tools used during development lifecycle to cover all interfaces.#2. Service Virtualisation tools used during development lifecycle but for only limited number of interfaces#3. Stubs created and used on need basis but no formal tool implemented#4. No usage of Service Virtuatlisation tools / Stubs|--None--#1. 100% of deployment process automated with no manual intervention for all build artefacts which are stored in a separate build repository and completed within a 4 hour timeframe.#2. 80% of deployment process automated with remaining covered through manual intervention.#3. Deployment script exists but only used for Dev and Test Environments.#4. Deployments done manually.|--None--#1. Subset of Automated test cases that cover the most important functionality of a component or system are run to ascertain if the most crucial functions of a program work correctly immediately as part of build verification.#2. Subset of Manual test cases that cover the most important functionality of a component or system are run to ascertain if the most crucial functions of a program work correctly immediately as part of build verification.#3. Smoke testing conducted on need basis but not as part of every build verfication.#4. Functional Smoke Test Cases non available.|--None--#1. Production Release can happen on Demand and usually happens more than once a month#2. Production Release happens monthly#3. Production Release every 3 months#4. Production Releases frequency > 3 months,Maturity Level of Practices[2]_lst,Score_cell
